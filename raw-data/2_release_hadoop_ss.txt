More moving stuff from core into common.    
Moving tags to new location.   
Tagging release 0.1.1.   
Merge -r 390737:392453 from trunk, preparing for 0.1.1 release.   
Update developer build version in branch.    
Starting a branch for 0.1 releases.   
More fixes to get working directory to work on Windows.  Long-term we should stop using java.io.File for our abstract file paths.  On Windows, 'new File(/foo/bar).isAbsolute()' returns false, which caused lots of problems.  So I have added a FileSystem.isAbsolute() method that we use internally.  I also added a few more .getAbsoluteFile() calls to convert paths into absolute paths so that LocalFileSystem works correctly.  Finally I took advantage of file status information cached in DFSFile, eliminating some namenode RPCs.   
Fix wiki url.    
If close() fails, then abandon the file, so that any leases are cleared and other task may attempt to create it.    
Fix a bug where writing zero-length files would cause things to hang.    
DFS re-format should fully delete old namenode data.   
Fix for HADOOP-102.  Contributed by Konstantin.    
Fix HADOOP-100.  Be more consistent about synchronization of access to taskTracker collection.  Contributed by Owen O'Malley.   
Fix for HADOOP-107.  As they were written, dfs blocks were both trickled to a datanode and tee'd to a temp file (in case the connection to the datanode failed).  Now they're only written to the temp file, with no connection to the datanode made until the block is complete.  This reduces the number of long-lived mostly-idle connections to datanodes, which was causing problems.  It also simplifies the DFSClient code significantly.   
Fix for HADOOP-103, part II: I forgot to add this file the first time!   
Fix for HADOOP-103.  Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable.  Use it in supplied Mappers & Reducers.  Also some minor improvements to demos.  Contributed by Owen O'Malley.   
Fix for HADOOP-110.  Reuse keys and values when mapping.  Contributed by Owen O'Malley.    
Fix for HADOOP-2.  The combiner now clones keys and values, so mappers may now safely reuse emitted keys and values.  Contributed by Owen O'Malley.   
Fix for HADOOP-112.  All operations on local files are now performed through a LocalFileSystem.  In particular, listing the local directory, which was causing this bug, when CRC files were included in the listing.  Now they are correctly excluded.    
Fix for HADOOP-84.  Improve error and log messages when block cannot be obtained by including file name and offset.  Also removed a few unused variables.  Contributed by Konstantin Shvachko.   
Fix HADOOP-33.  Avoid calling df too frequently by caching values internally.  Contributed by Konstantin Shvachko.   
Fix HADOOP-67.  Add a public API for dfs statistics.  Also switch to use the public API for reporting in DFSShell.   
Add a tool for checking DFS consistency (HADOOP-101).  Add a shortcut to bin/hadoop. In accordance with long-standing *nix tradition this command is called 'fsck'.  Development of this tool has been supported by Krugle.net. Thank you!    
Fix for file names with spaces.    
Always return an absolute pathname for local files.  This fixes problems on Windows, where a path specified with "/foo" in a config file is sometimes treated as a relative path.    
Fix unit tests on Windows.  Don't assume that, just because a pathname begins with a slash that it also returns true for File.isAbsolute().  Instead use getAbsoluteFile() to force such things to be absolute.   
Document the new format command.    
Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.   
Move checking of output directory existence from JobClient to OutputFormat, so that it can be overridden.  Add a base class for OutputFormat that implements this new method.   
Fix for HADOOP-46.  Jobs can now be named.  Contributed by Owen O'Malley.   
Fix for HADOOP-98.  Keep more accurate task counts.  Contributed by Owen O'Malley.   
Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.   
Fix for HADOOP-83.   
Much improved hadoop logo.  Contributed by Stefan.  Thanks!   
Fix for HADOOP-3.  Don't permit jobs to write to a pre-existing output directory.  Contributed by Owen O'Malley.   
Fix for HADOOP-45.  Fatal task errors are now logged at the JobTracker, facilitating debugging.    
Fix for HADOOP-44.  The error string for remote exceptions now contains the full remote stack trace.  Remote exceptions are now also re-thrown on the client as RemoteException rather than IOException, so that they can be distinguished from other IOExceptions.   
Start namenode before datanodes to minimize datanode startup errors.    
Fix for HADOOP-97.  Improve error handling.  Contributed by Konstantin Shvachko.    
Fix for HADOOP-87.  Dont' pass large buffers through to deflater as this is inefficient.    
Fix for HADOOP-93.  Convert min split size from int to long, and permit its specification in the config.   
Fix for HADOOP-86.  Errors while reading map output now cause map task to fail and be re-executed.   
Give server implementations access to a server's context.  This consists of two additions.  First is a static method Server.get() which returns the server instance it is called under, if any.  Second is the new public class RPC.Server, that replaces a former anonymous class.  RPC server implementation methods can now subclass RPC.Server to keep server state in the subclass.  Application code can then call Server.get() to access that state.  Note that Server.get() may be called under parameter deserialization and return value serialization methods as well, called before and after actual server method calls, respectively.   
Fix for HADOOP-82.  Completes count should never be less than zero.  Contributed by Michael Stack.   
Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.   
Fix for HADOOP-66.  Delete dfs temp files on JVM exit.   
Fix for HADOOP-80.  Make BytesWritable also a WritableComparable.  Also add hashBytes() utility method to WritableComparator and use it to hash both BytesWritable and UTF8.  Contributed by Owen O'Malley.   
Updated link to jira.   
Fix for HADOOP-79.  Some namenode optimizations.  Contributed by Konstantin Shvachko.   
Fix tasktracker to exit when errors are encountered reading map output, in order to force re-execution of map tasks.  It's overkill, since it will re-compute all map output computed by that task tracker, not just that which could not be read, but this should be a rare situation.  If we start seeing it frequently, then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re-executed.   
Fix for HADOOP-78.  Stream buffering was mistakenly disabled in writes by the RPC client.  Identified & fixed by Owen O'Malley.    
Use build/test for unit test data, instead of /tmp as in defaults.    
Fix for HADOOP-77.  Fixes some NPEs.  Contributed by Stefan.   
Fix for HADOOP-70.  Unit tests should have their own hadoop-site.xml and mapred-default.xml, so that local modifications to these files in conf/ don't alter unit testing.  Also rename TestDFS so that it is not normally run, and add a new test target which runs tests using the config files in conf/.   
Reverted changes from 384385, which removed local backup copy of block & removed most timeouts.  That worked well when all hosts are healthy, but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts.  So the local backup is back, but no longer in /tmp, rather in dfs.data.dir, and timeouts are back.  I also added connect timeouts, so that dfs connects also don't get hung up by slow hosts.   
Fixed wiki URL.   
Fixed wiki URL.   
Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.   
Add ability to sleep a bit between each command that's submitted to a slave, to meter slave commands a bit.   
Fix for HADOOP-57.  Permit listing of "/" in dfs.  With help from Mahadev konar.    
Apply local Configuration to parameters received via RPC if they implement Configurable. Fix by Marko Bauhardt.  Note: in the future there may be protocols that assume different local and remote Configuration. However, with the current RPC implementation we don't support it anyway, and for now it seems better that parameters implementing Configurable should be provided with non-null Configuration.    
Increase deflater & inflater buffer size, for better performance.    
Reduce iteration through all map & reduce tasks to improve jobtracker performance.    
Replace an NPE with an informative warning.    
Increase some intervals to further reduce stress on the jobtracker.    
Don't always query jobtracker for all needed map outputs, instead just for a random sample.  When the total number of splits was large, the jobtracker was spending most of its time servicing these requests. Also reduce the frequency of these requests.  Long-term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions.    
  The MapredLoadTest now has an extra step, to exercise the case where we have multiple reduce tasks.    It used to have two stages: one job that created a huge file of numbers in random order, followed by a job that would read that file and count the numbers.  If the final count was correct, the test passed.    Unfortunately, neither of these jobs had a reduce task that was greater than 1.    So now we've got three stages.  The first stage is unchanged.  The second stage reads the big file, then emits the answer key split into 10 parts, one for each reduce task.  then a third stage merges those parts into a final number count.  As before, if that final count is correct, all is well.     
  Fix bug HADOOP-26.  Available space is now considered correctly during DFS block-allocation.      
  This code makes sure we read from a local block, if available.  I thought this code had already been committed some time ago, but the workspace doesn't have it.     
Fix a couple of tasktracker bugs.   
  A bug in the TaskTracker was governing task-allocation by counting the total number of tasks.  The right thing to do is keep a total for map tasks, and a separate total for reduces.    This is now fixed.     
Check taskid's more carefully.  Suggested by Michael Stack.    
Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.   
Upgrade to Lucene 1.9.1.   
Fix for HADOOP-60, with help from Owen & Michael.   
Permit folks to modify options passed to ssh.  For example, older versions of ssh do not support the ConnectTimeout option.   
Add webapps to classpath so they're found.    
Fix rsync command to (a) use ssh as transport, and (b) to correctly quote the path.    
Fix to make independent of "unzip", using java's built in unzip code instead.    
Fix command line (again!).    
Update example command line.    
Un-escape containing jar's path, which is URL-encoded.  This fixes things primarily on Windows, where paths are likely to contain spaces.    
Escape paths so that spaces are permitted (as is common on Windows.)   
Upgrade lucene version to final release.   
Minor improvements to DOAP file.    
Add a DOAP description for hadoop.   
Print stack trace when child fails to contact parent.    
Fix for HADOOP-40.  Buffer size was ignored.   
Fix for HADOOP-41.  Support passing more options to child JVM.  Contributed by Michael Stack.   
Fix for HADOOP-40.  Buffer position was not maintained correctly.  Contributed by Konstantin Shvachko.   
Fix for HADOOP-49: Permit specification of jobtracker when submitting jobs.   
Fix so that task state is displayed even when there are no errors.  Also changed report to be a datastructure rather than a vector of strings.   
Updated javadoc for recent config changes.   
Add missing synchronization.   
Make speculative execution optional, since it can break map tasks that have side effects.  Also fix TestFileSystem to be safe to use with speculative execution.   
Fix HADOOP-36.  Scripts now source conf/hadoop-env.sh, to faciliate setting of environment variables, even on remote hosts.  The default slaves file has move from ~/.slaves to conf/slaves.   
Fix HADOOP-38: Add FileSystem.getBlockSize() method and use it as the maximum split size.  Also change FileSystem to implement Configurable, and improve some javadoc, using inherited comments where possible and removing implementation details from public javadoc.   
Move Closeable interface to io package, since it is of general utility, and to prepare for JDK 1.5.   
  Fix bug HADOOP-16.    Don't invoke TaskInProgress.hasTaskWithCacheHit() unnecessarily from within JobInProgress.    Also, cache filesystem hints inside JobInProgress.     
HADOOP-37: Add ClusterStatus.  Contributed by Owen O'Malley.   
Fix for HADOOP-34: make build paths relative to location of build.xml, not PWD.  Contributed by Jeremy Bensley.   
  Add a bunch of updated comments and JavaDocs to the Distributed File System package.     
Bundle webapps into jar.   
Fix HADOOP-30: add -lsr and -cat commands to DFSShell.  Contributed by Michel Tourn.   
Further improvements from Bryan A. Pendleton.    
Make compatible with JDK 1.4.    
Fix for HADOOP-12.  The JobTracker now loads the InputFormat from the job's jar file.   
Keep 'unzip' from prompting when overwriting (e.g., when archive contains same file twice).  Also make it less verbose.    
Add an easy way to specify a job's jar, by naming a class in the jar.   
Stop using deprecated 'jspc' ant task and instead call jasper directly.  Also rename webapp files to align with package name.   
Fixed HADOOP-20: permit mappers and reducers to cleanup.  Add a close() method to the Mapper and Reducer interfaces by having them extend a Closeable interface.  Update all implementations to define close().  Patch by Michel Tourn.   
Removed properties mistakenly copied from Nutch.    
Create html documentation for configuration defaults and link to these from javadoc.   
Fix for split from nutch.    
Fixed so that examples jar is packaged correctly.    
Fix HADOOP-28.  Jsp pages are now pre-compiled to servlets that can access package-private classes.   
Fix HADOOP-25: improve example code & package separately.  Contributed by Owen O'Malley.   
Updated javadoc.    
Initial version of a Hadoop tutorial.    
Clean up javadoc, reduce number of public classes.   
Log to logs directory by default.    
Remove a spurious space.    
Add WritableFactory mechanism, to permit one to use ObjectWritable with non-public classes.  Register factories for DFS implementation classes and make them non-public.  This greatly simplifies the javadoc.   
Improved javadoc, starting overview and package documentation.  Also moved DF from dfs to fs package.   
Add scripts into jar file so they're bundled with code.   
Fixed javadoc link and added more news.   
Fix for HADOOP-22: remove unused imports.  By Sami Siren.   
Fix HADOOP-21: fix mapred webapp jsp for split from Nutch.  Contributed by Owen O'Malley.   
Remove vestige of Nutch's build.xml so that nightly target will run.    
Fix HADOOP-5: add commons logging jar to lib.   
Remove a few vestiges of Nutch's plugin mechanism, fixing bug HADOOP-6 (Thanks, Owen!).  Also fix a few typos.    
Fix some config problems, remove notion of app resources: it's overkill.   
Don't require final resoureces.   
Permit multiple default and final Configuration resources.   
First version that passes unit tests.   
First version that compiles.   
Initial commit of code copied from Nutch.   
Initial commit of code copied from Nutch.   
Slightly improved logo.   
First version of website.   
Create hadoop sub-project.   
