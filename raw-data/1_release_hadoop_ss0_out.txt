Positive	Negative	Text	Explanation
1	-1	More moving stuff from core into common.    	More moving stuff from core into common .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Moving tags to new location.   	Moving tags to new location .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Tagging the 0.1.0 release.   	Tagging the 0 .[sentence: 1,-1] 1 .[sentence: 1,-1] 0 release .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Update developer build version in branch.    	Update developer build version in branch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Starting a branch for 0.1 releases.   	Starting a branch for 0 .[sentence: 1,-1] 1 releases .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	More fixes to get working directory to work on Windows.  Long-term we should stop using java.io.File for our abstract file paths.  On Windows, 'new File(/foo/bar).isAbsolute()' returns false, which caused lots of problems.  So I have added a FileSystem.isAbsolute() method that we use internally.  I also added a few more .getAbsoluteFile() calls to convert paths into absolute paths so that LocalFileSystem works correctly.  Finally I took advantage of file status information cached in DFSFile, eliminating some namenode RPCs.   	More fixes to get working directory to work on Windows .[sentence: 1,-1] Long -term we should stop using java .[sentence: 1,-1] io .[sentence: 1,-1] File for our abstract file paths .[sentence: 1,-1] On Windows ,'new File (/foo /bar ).[sentence: 1,-1] isAbsolute ()'returns false ,which caused lots of problems[-2].[sentence: 1,-2] So I have added a FileSystem .[sentence: 1,-1] isAbsolute ()method that we use internally .[sentence: 1,-1] I also added a few more .[sentence: 1,-1] getAbsoluteFile ()calls to convert paths into absolute paths so that LocalFileSystem works correctly .[sentence: 1,-1] Finally I took advantage of file status information cached in DFSFile ,eliminating some namenode RPCs .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix wiki url.    	Fix wiki url .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	If close() fails, then abandon the file, so that any leases are cleared and other task may attempt to create it.    	If close ()fails ,then abandon[-2]the file ,so that any leases are cleared and other task may attempt to create it .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-2	Fix a bug where writing zero-length files would cause things to hang.    	Fix a bug where writing zero -length files would cause things to hang[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	DFS re-format should fully delete old namenode data.   	DFS re -format should fully delete old namenode data .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-102.  Contributed by Konstantin.    	Fix for HADOOP -102 .[sentence: 1,-1] Contributed by Konstantin .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-100.  Be more consistent about synchronization of access to taskTracker collection.  Contributed by Owen O'Malley.   	Fix HADOOP -100 .[sentence: 1,-1] Be more consistent about synchronization of access to taskTracker collection .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-107.  As they were written, dfs blocks were both trickled to a datanode and tee'd to a temp file (in case the connection to the datanode failed).  Now they're only written to the temp file, with no connection to the datanode made until the block is complete.  This reduces the number of long-lived mostly-idle connections to datanodes, which was causing problems.  It also simplifies the DFSClient code significantly.   	Fix for HADOOP -107 .[sentence: 1,-1] As they were written ,dfs blocks were both trickled[-2]to a datanode and tee'd to a temp file (in case the connection to the datanode failed ).[sentence: 1,-2] Now they're only written to the temp file ,with no connection to the datanode made until the block is complete .[sentence: 1,-1] This reduces the number of long -lived mostly -idle connections to datanodes ,which was causing problems[-2].[sentence: 1,-2] It also simplifies the DFSClient code significantly .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-103, part II: I forgot to add this file the first time!   	Fix for HADOOP -103 ,part II :I forgot to add this file the first time ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-103.  Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable.  Use it in supplied Mappers & Reducers.  Also some minor improvements to demos.  Contributed by Owen O'Malley.   	Fix for HADOOP -103 .[sentence: 1,-1] Add a base class for Mapper and Reducer implementations that implements Closeable and JobConfigurable .[sentence: 1,-1] Use it in supplied Mappers &Reducers .[sentence: 1,-1] Also some minor improvements[2]to demos .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-110.  Reuse keys and values when mapping.  Contributed by Owen O'Malley.    	Fix for HADOOP -110 .[sentence: 1,-1] Reuse keys and values[2]when mapping .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix for HADOOP-2.  The combiner now clones keys and values, so mappers may now safely reuse emitted keys and values.  Contributed by Owen O'Malley.   	Fix for HADOOP -2 .[sentence: 1,-1] The combiner now clones keys and values[2],so mappers may now safely reuse emitted keys and values[2].[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-112.  All operations on local files are now performed through a LocalFileSystem.  In particular, listing the local directory, which was causing this bug, when CRC files were included in the listing.  Now they are correctly excluded.    	Fix for HADOOP -112 .[sentence: 1,-1] All operations on local files are now performed through a LocalFileSystem .[sentence: 1,-1] In particular ,listing the local directory ,which was causing this bug ,when CRC files were included in the listing .[sentence: 1,-1] Now they are correctly excluded .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix for HADOOP-84.  Improve error and log messages when block cannot be obtained by including file name and offset.  Also removed a few unused variables.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -84 .[sentence: 1,-1] Improve[2]error[-2]and log messages when block cannot be obtained by including file name and offset .[sentence: 2,-2] Also removed a few unused variables .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Fix HADOOP-33.  Avoid calling df too frequently by caching values internally.  Contributed by Konstantin Shvachko.   	Fix HADOOP -33 .[sentence: 1,-1] Avoid[-2]calling df too frequently by caching values[2]internally .[sentence: 2,-2] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-67.  Add a public API for dfs statistics.  Also switch to use the public API for reporting in DFSShell.   	Fix HADOOP -67 .[sentence: 1,-1] Add a public API for dfs statistics .[sentence: 1,-1] Also switch to use the public API for reporting in DFSShell .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-2	Add a tool for checking DFS consistency (HADOOP-101).  Add a shortcut to bin/hadoop. In accordance with long-standing *nix tradition this command is called 'fsck'.  Development of this tool has been supported by Krugle.net. Thank you!    	Add a tool for checking DFS consistency (HADOOP -101 ).[sentence: 1,-1] Add a shortcut to bin /hadoop .[sentence: 1,-1] In accordance with long -standing *nix[-2]tradition this command is called 'fsck '.[sentence: 1,-2] Development of this tool has been supported[2]by Krugle .[sentence: 2,-1] net .[sentence: 1,-1] Thank[2]you ![+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Fix for file names with spaces.    	Fix for file names with spaces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Always return an absolute pathname for local files.  This fixes problems on Windows, where a path specified with "/foo" in a config file is sometimes treated as a relative path.    	Always return an absolute pathname for local files .[sentence: 1,-1] This fixes problems[-2]on Windows ,where a path specified with '/foo 'in a config file is sometimes treated as a relative path .[sentence: 1,-2]  [result: max + and - of any sentence]
2	-2	Fix unit tests on Windows.  Don't assume that, just because a pathname begins with a slash that it also returns true for File.isAbsolute().  Instead use getAbsoluteFile() to force such things to be absolute.   	Fix unit tests on Windows .[sentence: 1,-1] Don't assume that ,just because a pathname begins with a slash[-2]that it also returns true[2]for File .[sentence: 2,-2] isAbsolute ().[sentence: 1,-1] Instead use getAbsoluteFile ()to force such things to be absolute .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Document the new format command.    	Document the new format command .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-3	Fix for HADOOP-19.  A namenode must now be formatted before it may be used.  Attempts to start a namenode in an unformatted directory will fail, rather than automatically creating a new, empty filesystem, causing existing datanodes to delete all blocks.  Thus a mis-configured dfs.data.dir should no longer cause data loss.   	Fix for HADOOP -19 .[sentence: 1,-1] A namenode must now be formatted before it may be used .[sentence: 1,-1] Attempts to start a namenode in an unformatted directory will fail ,rather than automatically creating a new ,empty filesystem ,causing existing datanodes to delete all blocks .[sentence: 1,-1] Thus a mis -configured dfs .[sentence: 1,-1] data .[sentence: 1,-1] dir should no longer cause data loss[-3].[sentence: 1,-3]  [result: max + and - of any sentence]
1	-1	Move checking of output directory existence from JobClient to OutputFormat, so that it can be overridden.  Add a base class for OutputFormat that implements this new method.   	Move checking of output directory existence from JobClient to OutputFormat ,so that it can be overridden .[sentence: 1,-1] Add a base class for OutputFormat that implements this new method .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-46.  Jobs can now be named.  Contributed by Owen O'Malley.   	Fix for HADOOP -46 .[sentence: 1,-1] Jobs can now be named .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-98.  Keep more accurate task counts.  Contributed by Owen O'Malley.   	Fix for HADOOP -98 .[sentence: 1,-1] Keep more accurate task counts .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-52.  Add username and working-directory to FileSystem and JobConf and use these to resolve relative paths.  Contributed by Owen O'Malley.   	Fix for HADOOP -52 .[sentence: 1,-1] Add username and working -directory to FileSystem and JobConf and use these to resolve relative paths .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-83.   	Fix for HADOOP -83 .[sentence: 1,-1]  [result: max + and - of any sentence]
3	-1	Much improved hadoop logo.  Contributed by Stefan.  Thanks!   	Much improved[2]hadoop logo .[sentence: 2,-1] Contributed by Stefan .[sentence: 1,-1] Thanks[2]![+0.6 punctuation emphasis][sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-3.  Don't permit jobs to write to a pre-existing output directory.  Contributed by Owen O'Malley.   	Fix for HADOOP -3 .[sentence: 1,-1] Don't permit jobs to write to a pre -existing output directory .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-45.  Fatal task errors are now logged at the JobTracker, facilitating debugging.    	Fix for HADOOP -45 .[sentence: 1,-1] Fatal task errors[-2]are now logged at the JobTracker ,facilitating debugging .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-44.  The error string for remote exceptions now contains the full remote stack trace.  Remote exceptions are now also re-thrown on the client as RemoteException rather than IOException, so that they can be distinguished from other IOExceptions.   	Fix for HADOOP -44 .[sentence: 1,-1] The error[-2]string for remote exceptions now contains the full remote stack trace .[sentence: 1,-2] Remote exceptions are now also re -thrown on the client as RemoteException rather than IOException ,so that they can be distinguished from other IOExceptions .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Start namenode before datanodes to minimize datanode startup errors.    	Start namenode before datanodes to minimize datanode startup errors[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
2	-2	Fix for HADOOP-97.  Improve error handling.  Contributed by Konstantin Shvachko.    	Fix for HADOOP -97 .[sentence: 1,-1] Improve[2]error[-2]handling .[sentence: 2,-2] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-87.  Dont' pass large buffers through to deflater as this is inefficient.    	Fix for HADOOP -87 .[sentence: 1,-1] Dont 'pass large buffers through to deflater as this is inefficient .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-93.  Convert min split size from int to long, and permit its specification in the config.   	Fix for HADOOP -93 .[sentence: 1,-1] Convert min split size from int to long ,and permit its specification in the config .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-86.  Errors while reading map output now cause map task to fail and be re-executed.   	Fix for HADOOP -86 .[sentence: 1,-1] Errors[-2]while reading map output now cause map task to fail and be re -executed .[sentence: 1,-2]  [result: max + and - of any sentence]
3	-1	Give server implementations access to a server's context.  This consists of two additions.  First is a static method Server.get() which returns the server instance it is called under, if any.  Second is the new public class RPC.Server, that replaces a former anonymous class.  RPC server implementation methods can now subclass RPC.Server to keep server state in the subclass.  Application code can then call Server.get() to access that state.  Note that Server.get() may be called under parameter deserialization and return value serialization methods as well, called before and after actual server method calls, respectively.   	Give server implementations access to a server's context .[sentence: 1,-1] This consists of two additions .[sentence: 1,-1] First is a static method Server .[sentence: 1,-1] get ()which returns the server instance it is called under ,if any .[sentence: 1,-1] Second is the new public class RPC .[sentence: 1,-1] Server ,that replaces a former anonymous class .[sentence: 1,-1] RPC server implementation methods can now subclass RPC .[sentence: 1,-1] Server to keep server state in the subclass .[sentence: 1,-1] Application code can then call Server .[sentence: 1,-1] get ()to access that state .[sentence: 1,-1] Note that Server .[sentence: 1,-1] get ()may be called under parameter deserialization and return value serialization methods as well ,called before and after actual server method calls ,respectively[3].[sentence: 3,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-82.  Completes count should never be less than zero.  Contributed by Michael Stack.   	Fix for HADOOP -82 .[sentence: 1,-1] Completes count should never be less than zero .[sentence: 1,-1] Contributed by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-81.  Job-specific parameters should be read from the job-specific configuration, not the daemon's.  This permits speculative execution, number of map & reduce tasks, etc. to be settable in the job.  Contributed by Owen O'Malley.   	Fix for HADOOP -81 .[sentence: 1,-1] Job -specific parameters should be read from the job -specific configuration ,not the daemon's .[sentence: 1,-1] This permits speculative execution ,number of map &reduce tasks ,etc .[sentence: 1,-1] to be settable in the job .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-66.  Delete dfs temp files on JVM exit.   	Fix for HADOOP -66 .[sentence: 1,-1] Delete dfs temp files on JVM exit[-2].[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-80.  Make BytesWritable also a WritableComparable.  Also add hashBytes() utility method to WritableComparator and use it to hash both BytesWritable and UTF8.  Contributed by Owen O'Malley.   	Fix for HADOOP -80 .[sentence: 1,-1] Make BytesWritable also a WritableComparable .[sentence: 1,-1] Also add hashBytes ()utility method to WritableComparator and use it to hash both BytesWritable and UTF8 .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated link to jira.   	Updated link to jira .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-79.  Some namenode optimizations.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -79 .[sentence: 1,-1] Some namenode optimizations .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix tasktracker to exit when errors are encountered reading map output, in order to force re-execution of map tasks.  It's overkill, since it will re-compute all map output computed by that task tracker, not just that which could not be read, but this should be a rare situation.  If we start seeing it frequently, then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re-executed.   	Fix tasktracker to exit[-2]when errors[-2]are encountered reading map output ,in order to force re -execution of map tasks .[sentence: 1,-2] It's overkill ,since it will re -compute all map output computed by that task tracker ,not just that which could not be read ,but this should be a rare situation .[sentence: 1,-1] If we start seeing it frequently ,then we could optimize this by adding a way to tell the jobtracker that a particular previously completed map task now needs to be re -executed .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix for HADOOP-78.  Stream buffering was mistakenly disabled in writes by the RPC client.  Identified & fixed by Owen O'Malley.    	Fix for HADOOP -78 .[sentence: 1,-1] Stream buffering was mistakenly[-2]disabled in writes by the RPC client .[sentence: 1,-2] Identified &fixed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Use build/test for unit test data, instead of /tmp as in defaults.    	Use build /test for unit test data ,instead of /tmp as in defaults .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-77.  Fixes some NPEs.  Contributed by Stefan.   	Fix for HADOOP -77 .[sentence: 1,-1] Fixes some NPEs .[sentence: 1,-1] Contributed by Stefan .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-70.  Unit tests should have their own hadoop-site.xml and mapred-default.xml, so that local modifications to these files in conf/ don't alter unit testing.  Also rename TestDFS so that it is not normally run, and add a new test target which runs tests using the config files in conf/.   	Fix for HADOOP -70 .[sentence: 1,-1] Unit tests should have their own hadoop -site .[sentence: 1,-1] xml and mapred -default .[sentence: 1,-1] xml ,so that local modifications to these files in conf /don't alter unit testing .[sentence: 1,-1] Also rename TestDFS so that it is not normally run ,and add a new test target which runs tests using the config files in conf /.[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Reverted changes from 384385, which removed local backup copy of block & removed most timeouts.  That worked well when all hosts are healthy, but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts.  So the local backup is back, but no longer in /tmp, rather in dfs.data.dir, and timeouts are back.  I also added connect timeouts, so that dfs connects also don't get hung up by slow hosts.   	Reverted changes from 384385 ,which removed local backup copy of block &removed most timeouts .[sentence: 1,-1] That worked well when all hosts are healthy ,but when a few are very slow it caused too many tasks to timeout and loads to balloon on slow hosts .[sentence: 1,-1] So the local backup is back ,but no longer in /tmp ,rather in dfs .[sentence: 1,-1] data .[sentence: 1,-1] dir ,and timeouts are back .[sentence: 1,-1] I also added connect timeouts ,so that dfs connects also don't get hung up by slow hosts .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed wiki URL.   	Fixed wiki URL .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed wiki URL.   	Fixed wiki URL .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-66.  DFS blocks are no longer written to local temp files.  If a connection to a datanode fails then an exception is now thrown, rather than trying to re-connect to another datanode.  Timeouts were also removed from datanode connections, since these caused a lot of failed connections.   	Fix for HADOOP -66 .[sentence: 1,-1] DFS blocks are no longer written to local temp files .[sentence: 1,-1] If a connection to a datanode fails then an exception is now thrown ,rather than trying to re -connect to another datanode .[sentence: 1,-1] Timeouts were also removed from datanode connections ,since these caused a lot of failed connections .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add ability to sleep a bit between each command that's submitted to a slave, to meter slave commands a bit.   	Add ability to sleep a bit between each command that's submitted to a slave ,to meter slave commands a bit .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-57.  Permit listing of "/" in dfs.  With help from Mahadev konar.    	Fix for HADOOP -57 .[sentence: 1,-1] Permit listing of '/'in dfs .[sentence: 1,-1] With help from Mahadev konar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Apply local Configuration to parameters received via RPC if they implement Configurable. Fix by Marko Bauhardt.  Note: in the future there may be protocols that assume different local and remote Configuration. However, with the current RPC implementation we don't support it anyway, and for now it seems better that parameters implementing Configurable should be provided with non-null Configuration.    	Apply local Configuration to parameters received via RPC if they implement Configurable .[sentence: 1,-1] Fix by Marko Bauhardt .[sentence: 1,-1] Note :in the future there may be protocols that assume different local and remote Configuration .[sentence: 1,-1] However ,with the current RPC implementation we don't support it anyway ,and for now it seems better that parameters implementing Configurable should be provided with non -null Configuration .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Increase deflater & inflater buffer size, for better performance.    	Increase deflater &inflater buffer size ,for better performance .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Reduce iteration through all map & reduce tasks to improve jobtracker performance.    	Reduce iteration through all map &reduce tasks to improve[2]jobtracker performance .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Replace an NPE with an informative warning.    	Replace an NPE with an informative warning .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Increase some intervals to further reduce stress on the jobtracker.    	Increase some intervals to further reduce stress[-2]on the jobtracker .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Don't always query jobtracker for all needed map outputs, instead just for a random sample.  When the total number of splits was large, the jobtracker was spending most of its time servicing these requests. Also reduce the frequency of these requests.  Long-term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions.    	Don't always query jobtracker for all needed map outputs ,instead just for a random sample .[sentence: 1,-1] When the total number of splits was large ,the jobtracker was spending most of its time servicing these requests .[sentence: 1,-1] Also reduce the frequency of these requests .[sentence: 1,-1] Long -term we may need a different algorithm here to ensure that reduces are more promptly and efficiently notified of map completions .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	  The MapredLoadTest now has an extra step, to exercise the case where we have multiple reduce tasks.    It used to have two stages: one job that created a huge file of numbers in random order, followed by a job that would read that file and count the numbers.  If the final count was correct, the test passed.    Unfortunately, neither of these jobs had a reduce task that was greater than 1.    So now we've got three stages.  The first stage is unchanged.  The second stage reads the big file, then emits the answer key split into 10 parts, one for each reduce task.  then a third stage merges those parts into a final number count.  As before, if that final count is correct, all is well.     	The MapredLoadTest now has an extra step ,to exercise the case where we have multiple reduce tasks .[sentence: 1,-1] It used to have two stages :one job that created a huge file of numbers in random order ,followed by a job that would read that file and count the numbers .[sentence: 1,-1] If the final count was correct ,the test passed .[sentence: 1,-1] Unfortunately[-2],neither of these jobs had a reduce task that was greater than 1 .[sentence: 1,-2] So now we've got three stages .[sentence: 1,-1] The first stage is unchanged .[sentence: 1,-1] The second stage reads the big file ,then emits the answer key split into 10 parts ,one for each reduce task .[sentence: 1,-1] then a third stage merges those parts into a final number count .[sentence: 1,-1] As before ,if that final count is correct ,all is well .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	  Fix bug HADOOP-26.  Available space is now considered correctly during DFS block-allocation.      	Fix bug HADOOP -26 .[sentence: 1,-1] Available space is now considered correctly during DFS block -allocation .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	  This code makes sure we read from a local block, if available.  I thought this code had already been committed some time ago, but the workspace doesn't have it.     	This code makes sure we read from a local block ,if available .[sentence: 1,-1] I thought this code had already been committed some time ago ,but the workspace doesn't have it .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix a couple of tasktracker bugs.   	Fix a couple of tasktracker bugs .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	  A bug in the TaskTracker was governing task-allocation by counting the total number of tasks.  The right thing to do is keep a total for map tasks, and a separate total for reduces.    This is now fixed.     	A bug in the TaskTracker was governing task -allocation by counting the total number of tasks .[sentence: 1,-1] The right thing to do is keep a total for map tasks ,and a separate total for reduces .[sentence: 1,-1] This is now fixed .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Check taskid's more carefully.  Suggested by Michael Stack.    	Check taskid's more carefully .[sentence: 1,-1] Suggested by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-16.  Splitting and other job planning is now performed in a separate thread.   	Fix for HADOOP -16 .[sentence: 1,-1] Splitting and other job planning is now performed in a separate thread .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Upgrade to Lucene 1.9.1.   	Upgrade to Lucene 1 .[sentence: 1,-1] 9 .[sentence: 1,-1] 1 .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-60, with help from Owen & Michael.   	Fix for HADOOP -60 ,with help from Owen &Michael .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Permit folks to modify options passed to ssh.  For example, older versions of ssh do not support the ConnectTimeout option.   	Permit folks to modify options passed to ssh .[sentence: 1,-1] For example ,older versions of ssh do not support the ConnectTimeout option .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add webapps to classpath so they're found.    	Add webapps to classpath so they're found .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix rsync command to (a) use ssh as transport, and (b) to correctly quote the path.    	Fix rsync command to (a )use ssh as transport ,and (b )to correctly quote the path .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix to make independent of "unzip", using java's built in unzip code instead.    	Fix to make independent of 'unzip ',using java's built in unzip code instead .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix command line (again!).    	Fix command line (again !).[+0.6 punctuation mood emphasis][sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Update example command line.    	Update example command line .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Un-escape containing jar's path, which is URL-encoded.  This fixes things primarily on Windows, where paths are likely to contain spaces.    	Un -escape containing jar's path ,which is URL -encoded .[sentence: 1,-1] This fixes things primarily on Windows ,where paths are likely to contain spaces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Escape paths so that spaces are permitted (as is common on Windows.)   	Escape paths so that spaces are permitted (as is common on Windows .)[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Upgrade lucene version to final release.   	Upgrade lucene version to final release .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Minor improvements to DOAP file.    	Minor improvements[2]to DOAP file .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Add a DOAP description for hadoop.   	Add a DOAP description for hadoop .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Print stack trace when child fails to contact parent.    	Print stack trace when child fails to contact parent .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-40.  Buffer size was ignored.   	Fix for HADOOP -40 .[sentence: 1,-1] Buffer size was ignored .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-41.  Support passing more options to child JVM.  Contributed by Michael Stack.   	Fix for HADOOP -41 .[sentence: 1,-1] Support passing more options to child JVM .[sentence: 1,-1] Contributed by Michael Stack .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-40.  Buffer position was not maintained correctly.  Contributed by Konstantin Shvachko.   	Fix for HADOOP -40 .[sentence: 1,-1] Buffer position was not maintained correctly .[sentence: 1,-1] Contributed by Konstantin Shvachko .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-49: Permit specification of jobtracker when submitting jobs.   	Fix for HADOOP -49 :Permit specification of jobtracker when submitting jobs .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix so that task state is displayed even when there are no errors.  Also changed report to be a datastructure rather than a vector of strings.   	Fix so that task state is displayed even when there are no errors[-2].[sentence: 1,-2] Also changed report to be a datastructure rather than a vector of strings .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated javadoc for recent config changes.   	Updated javadoc for recent config changes .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add missing synchronization.   	Add missing synchronization .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Make speculative execution optional, since it can break map tasks that have side effects.  Also fix TestFileSystem to be safe to use with speculative execution.   	Make speculative execution optional ,since it can break map tasks that have side effects .[sentence: 1,-1] Also fix TestFileSystem to be safe to use with speculative execution .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix HADOOP-36.  Scripts now source conf/hadoop-env.sh, to faciliate setting of environment variables, even on remote hosts.  The default slaves file has move from ~/.slaves to conf/slaves.   	Fix HADOOP -36 .[sentence: 1,-1] Scripts now source[-2]conf /hadoop -env .[sentence: 1,-2] sh ,to faciliate setting of environment variables ,even on remote hosts .[sentence: 1,-1] The default slaves file has move from ~/.[sentence: 1,-1] slaves to conf /slaves .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix HADOOP-38: Add FileSystem.getBlockSize() method and use it as the maximum split size.  Also change FileSystem to implement Configurable, and improve some javadoc, using inherited comments where possible and removing implementation details from public javadoc.   	Fix HADOOP -38 :Add FileSystem .[sentence: 1,-1] getBlockSize ()method and use it as the maximum split size .[sentence: 1,-1] Also change FileSystem to implement Configurable ,and improve[2]some javadoc ,using inherited comments where possible and removing implementation details from public javadoc .[sentence: 2,-1]  [result: max + and - of any sentence]
1	-1	Move Closeable interface to io package, since it is of general utility, and to prepare for JDK 1.5.   	Move Closeable interface to io package ,since it is of general utility ,and to prepare for JDK 1 .[sentence: 1,-1] 5 .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	  Fix bug HADOOP-16.    Don't invoke TaskInProgress.hasTaskWithCacheHit() unnecessarily from within JobInProgress.    Also, cache filesystem hints inside JobInProgress.     	Fix bug HADOOP -16 .[sentence: 1,-1] Don't invoke TaskInProgress .[sentence: 1,-1] hasTaskWithCacheHit ()unnecessarily[-2]from within JobInProgress .[sentence: 1,-2] Also ,cache filesystem hints inside JobInProgress .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	HADOOP-37: Add ClusterStatus.  Contributed by Owen O'Malley.   	HADOOP -37 :Add ClusterStatus .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-34: make build paths relative to location of build.xml, not PWD.  Contributed by Jeremy Bensley.   	Fix for HADOOP -34 :make build paths relative to location of build .[sentence: 1,-1] xml ,not PWD .[sentence: 1,-1] Contributed by Jeremy Bensley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	  Add a bunch of updated comments and JavaDocs to the Distributed File System package.     	Add a bunch of updated comments and JavaDocs to the Distributed File System package .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Bundle webapps into jar.   	Bundle webapps into jar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-30: add -lsr and -cat commands to DFSShell.  Contributed by Michel Tourn.   	Fix HADOOP -30 :add -lsr and -cat commands to DFSShell .[sentence: 1,-1] Contributed by Michel Tourn .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Further improvements from Bryan A. Pendleton.    	Further improvements[2]from Bryan A .[sentence: 2,-1] Pendleton .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Make compatible with JDK 1.4.    	Make compatible with JDK 1 .[sentence: 1,-1] 4 .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-12.  The JobTracker now loads the InputFormat from the job's jar file.   	Fix for HADOOP -12 .[sentence: 1,-1] The JobTracker now loads the InputFormat from the job's jar file .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Keep 'unzip' from prompting when overwriting (e.g., when archive contains same file twice).  Also make it less verbose.    	Keep 'unzip 'from prompting when overwriting (e .[sentence: 1,-1] g .,[sentence: 1,-1] when archive contains same file twice ).[sentence: 1,-1] Also make it less verbose .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add an easy way to specify a job's jar, by naming a class in the jar.   	Add an easy way to specify a job's jar ,by naming a class in the jar .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Stop using deprecated 'jspc' ant task and instead call jasper directly.  Also rename webapp files to align with package name.   	Stop using deprecated 'jspc 'ant task and instead call jasper directly .[sentence: 1,-1] Also rename webapp files to align with package name .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed HADOOP-20: permit mappers and reducers to cleanup.  Add a close() method to the Mapper and Reducer interfaces by having them extend a Closeable interface.  Update all implementations to define close().  Patch by Michel Tourn.   	Fixed HADOOP -20 :permit mappers and reducers to cleanup .[sentence: 1,-1] Add a close ()method to the Mapper and Reducer interfaces by having them extend a Closeable interface .[sentence: 1,-1] Update all implementations to define close ().[sentence: 1,-1] Patch by Michel Tourn .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Removed properties mistakenly copied from Nutch.    	Removed properties mistakenly[-2]copied from Nutch .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Create html documentation for configuration defaults and link to these from javadoc.   	Create html documentation for configuration defaults and link to these from javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for split from nutch.    	Fix for split from nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed so that examples jar is packaged correctly.    	Fixed so that examples jar is packaged correctly .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-28.  Jsp pages are now pre-compiled to servlets that can access package-private classes.   	Fix HADOOP -28 .[sentence: 1,-1] Jsp pages are now pre -compiled to servlets that can access package -private classes .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Fix HADOOP-25: improve example code & package separately.  Contributed by Owen O'Malley.   	Fix HADOOP -25 :improve[2]example code &package separately .[sentence: 2,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Updated javadoc.    	Updated javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial version of a Hadoop tutorial.    	Initial version of a Hadoop tutorial .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Clean up javadoc, reduce number of public classes.   	Clean up javadoc ,reduce number of public classes .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Log to logs directory by default.    	Log to logs directory by default .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Remove a spurious space.    	Remove a spurious space .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add WritableFactory mechanism, to permit one to use ObjectWritable with non-public classes.  Register factories for DFS implementation classes and make them non-public.  This greatly simplifies the javadoc.   	Add WritableFactory mechanism ,to permit one to use ObjectWritable with non -public classes .[sentence: 1,-1] Register factories for DFS implementation classes and make them non -public .[sentence: 1,-1] This greatly simplifies the javadoc .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Improved javadoc, starting overview and package documentation.  Also moved DF from dfs to fs package.   	Improved[2]javadoc ,starting overview and package documentation .[sentence: 2,-1] Also moved DF from dfs to fs package .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Add scripts into jar file so they're bundled with code.   	Add scripts into jar file so they're bundled with code .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fixed javadoc link and added more news.   	Fixed javadoc link and added more news .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix for HADOOP-22: remove unused imports.  By Sami Siren.   	Fix for HADOOP -22 :remove unused imports .[sentence: 1,-1] By Sami Siren .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-21: fix mapred webapp jsp for split from Nutch.  Contributed by Owen O'Malley.   	Fix HADOOP -21 :fix mapred webapp jsp for split from Nutch .[sentence: 1,-1] Contributed by Owen O'Malley .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Remove vestige of Nutch's build.xml so that nightly target will run.    	Remove vestige of Nutch's build .[sentence: 1,-1] xml so that nightly target will run .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Fix HADOOP-5: add commons logging jar to lib.   	Fix HADOOP -5 :add commons logging jar to lib .[sentence: 1,-1]  [result: max + and - of any sentence]
2	-1	Remove a few vestiges of Nutch's plugin mechanism, fixing bug HADOOP-6 (Thanks, Owen!).  Also fix a few typos.    	Remove a few vestiges of Nutch's plugin mechanism ,fixing bug HADOOP -6 (Thanks[2],Owen !).[+0.6 punctuation mood emphasis][sentence: 2,-1] Also fix a few typos .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-2	Fix some config problems, remove notion of app resources: it's overkill.   	Fix some config problems[-2],remove notion of app resources :it's overkill .[sentence: 1,-2]  [result: max + and - of any sentence]
1	-1	Don't require final resoureces.   	Don't require final resoureces .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Permit multiple default and final Configuration resources.   	Permit multiple default and final Configuration resources .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version that passes unit tests.   	First version that passes unit tests .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version that compiles.   	First version that compiles .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial commit of code copied from Nutch.   	Initial commit of code copied from Nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Initial commit of code copied from Nutch.   	Initial commit of code copied from Nutch .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Slightly improved logo.   	Slightly improved[2][+-1 booster word]logo .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	First version of website.   	First version of website .[sentence: 1,-1]  [result: max + and - of any sentence]
1	-1	Create hadoop sub-project.   	Create hadoop sub -project .[sentence: 1,-1]  [result: max + and - of any sentence]
